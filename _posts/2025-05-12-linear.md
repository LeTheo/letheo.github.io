---
layout: post
title:  "선형회귀"
categories: 딥러닝
---


**1. 선형 회귀란 무엇일까요?**

선형 회귀는 가장 기본적인 머신러닝/통계 모델 중 하나입니다. **하나 이상의 독립 변수(입력 특징, $x$)와 종속 변수(예측하려는 값, $y$) 사이의 관계를 가장 잘 나타내는 '직선' (또는 고차원에서는 '초평면')을 찾는 방법**이라고 생각하시면 됩니다.

예를 들어, 공부 시간($x$)에 따른 시험 점수($y$)를 예측한다고 해봅시다. 선형 회귀는 공부 시간과 시험 점수 데이터들을 가장 잘 설명하는 하나의 직선 방정식을 찾는 과정입니다.

**2. 어떻게 작동하나요? (모델 형태와 학습)**

* **모델 형태:** 선형 회귀 모델은 기본적으로 다음과 같은 **선형 방정식** 형태를 가집니다.
    $$y \approx Wx + b$$
    * $y$: 우리가 예측하려는 값 (종속 변수, 예: 시험 점수)
    * $x$: 예측에 사용하는 값 (독립 변수, 예: 공부 시간)
    * $W$: **가중치(Weight)** 또는 **기울기(Slope)**. 입력($x$)이 예측값($y$)에 얼마나 영향을 미치는지를 나타냅니다.
    * $b$: **편향(Bias)** 또는 **절편(Intercept)**. 입력($x$)이 0일 때의 예측값입니다.

    만약 독립 변수가 여러 개라면 (예: 공부 시간, 수면 시간으로 시험 점수 예측) 다중 선형 회귀가 되며, 모델은 $y \approx W_1 x_1 + W_2 x_2 + \dots + W_n x_n + b$ 와 같이 확장됩니다.

* **학습 과정:** 선형 회귀 모델을 '학습'한다는 것은, 주어진 데이터($x, y$ 쌍들)를 가장 잘 설명하는 최적의 가중치($W$)와 편향($b$) 값을 찾는 과정입니다. "가장 잘 설명한다"는 것은 일반적으로 **실제 값($y$)과 모델의 예측 값($Wx+b$) 사이의 오차를 최소화**하는 것을 의미합니다.
    * 이 오차를 측정하는 함수를 **손실 함수(Loss function)** 또는 비용 함수(Cost function)라고 부릅니다. 선형 회귀에서는 주로 **평균 제곱 오차(Mean Squared Error, MSE)** 가 사용됩니다. MSE는 실제 값과 예측 값의 차이를 제곱하여 평균낸 값입니다.
    * 이 손실 함수(MSE)를 최소화하는 $W$와 $b$를 찾기 위해 **경사 하강법(Gradient Descent)** 과 같은 최적화 알고리즘을 사용합니다. (이는 "[딥러닝] 1-2강. 경사 하강법 vs Newton's method" 에서 더 자세히 다뤄집니다.)

**3. 핵심 요약**

* **선형 회귀:** 독립 변수와 종속 변수 간의 관계를 직선으로 모델링하는 방법.
* **모델:** $y \approx Wx + b$ 형태로, 데이터에 가장 잘 맞는 가중치($W$)와 편향($b$)을 찾음.
* **학습:** 손실 함수(주로 MSE)를 정의하고, 이를 최소화하는 $W, b$를 경사 하강법 등으로 탐색.

**4. 복습 퀴즈**

1.  선형 회귀 모델 $y = Wx + b$ 에서 $W$와 $b$는 각각 무엇을 의미하며, 모델 학습의 목표는 무엇인가요?
2.  선형 회귀는 어떤 종류의 문제를 해결하는 데 사용될 수 있을까요? (예측 vs 분류) 실제 예시를 1~2가지 들어보세요.
3.  선형 회귀 모델의 학습 과정에서 '손실 함수'는 어떤 역할을 하며, 주로 어떤 함수가 사용되나요?

**5. 심화 학습**

* 선형 회귀에서 '선형'이 의미하는 더 깊은 뜻은 무엇일까요? ([딥러닝] 1-4강. 선형 회귀에서 "선형"의 찐의미)
* 선형 회귀 모델의 성능을 평가하는 지표에는 MSE 외에 어떤 것들이 있을까요? (MAE, R-squared 등)
* 독립 변수가 여러 개인 '다중 선형 회귀'에 대해 알아보세요.

선형 회귀에 대해 더 궁금한 점이 있으시면 언제든지 질문해주세요!